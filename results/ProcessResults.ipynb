{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bongard has 392 examples\n",
      "krk has 200 examples\n",
      "train has 400 examples\n",
      "samegen has 200 examples\n",
      "university has 27 examples\n",
      "mutag has 188 examples\n",
      "nci has 200 examples\n",
      "cancer has 330 examples\n",
      "financial has 234 examples\n",
      "ptc has 343 examples\n"
     ]
    }
   ],
   "source": [
    "problems = [\"bongard\",\"krk\",\"train\",\"samegen\",\"university\",\"mutag\",\"nci\",\"cancer\",\"financial\",\"ptc\"]\n",
    "base_path = os.path.join(\"docker\",\"Benchmark\")\n",
    "\n",
    "for problem in problems:\n",
    "    df = pd.read_csv(os.path.join(base_path,problem,\"relational\",f\"{problem}.csv\"))\n",
    "    print(f\"{problem} has {len(df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. nodes & 59 & 26 & 27 & 58 & 114 & 37 & 40 & 117 & 83 & 27 & 34 & 83 & 404 & 395 & 0 & 400 & 54 & 27 & 31 & 81 & \\\\\n",
      "Avg. edges & 3635 & 56 & 108 & 151 & 16217 & 160 & 234 & 320 & 22471 & 111 & 166 & 223 & 354293 & 8 & 0 & 15 & 8019 & 108 & 162 & 217 & \\\\\n",
      "Size & \\multicolumn{4}{c|}{148} & \\multicolumn{4}{c|}{160} & \\multicolumn{4}{c|}{260} & \\multicolumn{4}{c|}{184} & \\multicolumn{4}{c|}{270} & \\\\\n",
      "Depth & 1 & 9 & 2 & 12 & 1 & 13 & 5 & 27 & 1 & 9 & 7 & 18 & "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m depths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m graphs:\n\u001b[0;32m---> 63\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[43mto_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_undirected()\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m#depth = nx.diameter(G)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         depths\u001b[38;5;241m.\u001b[39mappend(nx\u001b[38;5;241m.\u001b[39mdiameter(G))\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/torch_geometric/utils/convert.py:174\u001b[0m, in \u001b[0;36mto_networkx\u001b[0;34m(data, node_attrs, edge_attrs, graph_attrs, to_undirected, remove_self_loops)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m edge_attrs \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m    172\u001b[0m             attr[key] \u001b[38;5;241m=\u001b[39m to_networkx_value(store[key][i])\n\u001b[0;32m--> 174\u001b[0m         \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/networkx/classes/digraph.py:710\u001b[0m, in \u001b[0;36mDiGraph.add_edge\u001b[0;34m(self, u_of_edge, v_of_edge, **attr)\u001b[0m\n\u001b[1;32m    708\u001b[0m datadict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u]\u001b[38;5;241m.\u001b[39mget(v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_attr_dict_factory())\n\u001b[1;32m    709\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(attr)\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_succ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m datadict\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pred[v][u] \u001b[38;5;241m=\u001b[39m datadict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the dataset features latex table: features as row and the datasets with representations as columns\n",
    "datasets = [\"mutag\",\"nci\",\"cancer\",\"financial\",\"ptc\"]\n",
    "#datasets = [\"mutag\"]\n",
    "#datasets = [\"krk\",\"bongard\",\"train\",\"sameGen\",\"cyclic\"]\n",
    "\n",
    "# average amount of nodes per example\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "\n",
    "# average amount of nodes\n",
    "print(\"Avg. nodes\",end=\" & \")\n",
    "for dataset in datasets:\n",
    "    all_nodes = {}\n",
    "    for representation in representations:\n",
    "        # if exists, load the graph data\n",
    "        if os.path.exists(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\")):\n",
    "            graphs = torch.load(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\"))\n",
    "            nodes = sum([len(graph.x) for graph in graphs])/len(graphs)\n",
    "            all_nodes[representation] = nodes\n",
    "        else:\n",
    "            all_nodes[representation] = 0\n",
    "    \n",
    "    values = list(all_nodes.values())\n",
    "    for value in values:\n",
    "        print(f\"{round(value)}\",end=\" & \")\n",
    "print(\"\\\\\\\\\")\n",
    "\n",
    "# average amount of edges\n",
    "print(\"Avg. edges\",end=\" & \")\n",
    "for dataset in datasets:\n",
    "    all_edges = {}\n",
    "    for representation in representations:\n",
    "        # if exists, load the graph data\n",
    "        if os.path.exists(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\")):\n",
    "            graphs = torch.load(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\"))\n",
    "            edges = sum([len(graph.edge_index[0]) for graph in graphs])/len(graphs)\n",
    "            all_edges[representation] = edges\n",
    "        else:\n",
    "            all_edges[representation] = 0\n",
    "    \n",
    "    values = list(all_edges.values())\n",
    "    for value in values:\n",
    "        print(f\"{round(value)}\",end=\" & \")\n",
    "print(\"\\\\\\\\\")\n",
    "\n",
    "# amount of examples, same value for all representations so use multicolumn \n",
    "print(\"Size\",end=\" & \")\n",
    "for dataset in datasets:\n",
    "    graphs = torch.load(os.path.join(base_path,dataset,\"node_only\",\"graph\",\"train.pt\"))\n",
    "    size = len(graphs)\n",
    "    print(\"\\multicolumn{4}{c|}{\"+str(size)+\"}\",end=\" & \")\n",
    "print(\"\\\\\\\\\")\n",
    "\n",
    "# Graph diameter -> do this with networkx\n",
    "print(\"Depth\",end=\" & \")\n",
    "for dataset in datasets:\n",
    "    all_depths = {}\n",
    "    for representation in representations:\n",
    "        # if exists, load the graph data\n",
    "        if os.path.exists(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\")):\n",
    "            graphs = torch.load(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\"))\n",
    "            depths = []\n",
    "            for graph in graphs:\n",
    "                G = to_networkx(graph).to_undirected()\n",
    "                try:\n",
    "                    #depth = nx.diameter(G)\n",
    "                    depths.append(nx.diameter(G))\n",
    "                except:\n",
    "                    #S = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "                    S = G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
    "                    #depths = [nx.diameter(s) for s in S]\n",
    "                    depths.append(nx.diameter(S))\n",
    "                #depths.append(max(depths))\n",
    "            all_depths[representation] = sum(depths)/len(depths)\n",
    "        else:\n",
    "            all_depths[representation] = 0\n",
    "    \n",
    "    values = list(all_depths.values())\n",
    "    for value in values:\n",
    "        print(f\"{round(value)}\",end=\" & \")\n",
    "print(\"\\\\\\\\\")\n",
    "    \n",
    "# Graph width -> # node features\n",
    "print(\"Width\",end=\" & \")\n",
    "for dataset in datasets:\n",
    "    all_widths = {}\n",
    "    for representation in representations:\n",
    "        # if exists, load the graph data\n",
    "        if os.path.exists(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\")):\n",
    "            graphs = torch.load(os.path.join(base_path,dataset,representation,\"graph\",\"train.pt\"))\n",
    "            widths = sum([len(graph.x[0]) for graph in graphs])/len(graphs)\n",
    "            all_widths[representation] = widths\n",
    "        else:\n",
    "            all_widths[representation] = 0\n",
    "    \n",
    "    values = list(all_widths.values())\n",
    "    for value in values:\n",
    "        print(f\"{round(value)}\",end=\" & \")\n",
    "print(\"\\\\\\\\\")\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: bongard\n",
      "{'node_only': 15.70873786407767, 'node_edge': 1.7864077669902914, 'Klog': 3.5728155339805827}\n",
      "Max edges: 15.70873786407767\n",
      "Min edges: 1.7864077669902914\n",
      "Problem: krk\n",
      "{'node_only': 3.0, 'node_edge': 3.0, 'Klog': 7.7594936708860756}\n",
      "Max edges: 7.7594936708860756\n",
      "Min edges: 3.0\n",
      "Problem: train\n",
      "{'node_only': 36.52215189873418, 'node_edge': 7.674050632911392, 'Klog': 15.348101265822784}\n",
      "Max edges: 36.52215189873418\n",
      "Min edges: 7.674050632911392\n",
      "Problem: samegen\n",
      "{'node_only': 1.0, 'node_edge': 39.56962025316456, 'Klog': 78.39240506329114}\n",
      "Max edges: 78.39240506329114\n",
      "Min edges: 1.0\n",
      "Problem: university\n",
      "{'node_only': 41.333333333333336, 'node_edge': 2.761904761904762, 'Klog': 2.761904761904762}\n",
      "Max edges: 41.333333333333336\n",
      "Min edges: 2.761904761904762\n",
      "Problem: mutag\n",
      "{'node_only': 1817.587837837838, 'node_edge': 28.006756756756758, 'Klog': 75.74324324324324}\n",
      "Max edges: 1817.587837837838\n",
      "Min edges: 28.006756756756758\n",
      "Problem: nci\n",
      "{'node_only': 8108.3625, 'node_edge': 79.9746835443038, 'Klog': 159.9493670886076}\n",
      "Max edges: 8108.3625\n",
      "Min edges: 79.9746835443038\n",
      "Problem: cancer\n",
      "{'node_only': 11235.684615384615, 'node_edge': 55.63846153846154, 'Klog': 111.27692307692308}\n",
      "Max edges: 11235.684615384615\n",
      "Min edges: 55.63846153846154\n",
      "Problem: financial\n",
      "{'node_only': 173350.22459893048, 'node_edge': 3.786096256684492, 'Klog': 7.572192513368984}\n",
      "Max edges: 173350.22459893048\n",
      "Min edges: 3.786096256684492\n",
      "Problem: ptc\n",
      "{'node_only': 4186.080291970803, 'node_edge': 54.22222222222222, 'Klog': 108.44444444444444}\n",
      "Max edges: 4186.080291970803\n",
      "Min edges: 54.22222222222222\n"
     ]
    }
   ],
   "source": [
    "# average amount of edges per example\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "representations = [\"node_only\",\"node_edge\",\"Klog\"]\n",
    "for problem in problems:\n",
    "    print(f\"Problem: {problem}\")\n",
    "    all_edges = {}\n",
    "    for representation in representations:\n",
    "        graphs = torch.load(os.path.join(base_path,problem,representation,\"graph\",\"train.pt\"))\n",
    "        edges = sum([len(graph.edge_index[0])/2 for graph in graphs])/len(graphs)\n",
    "        all_edges[representation] = edges\n",
    "    print(all_edges)\n",
    "    values = list(all_edges.values())\n",
    "    print(f\"Max edges: {max(values)}\")\n",
    "    print(f\"Min edges: {min(values)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logic_results(problem):\n",
    "    base_path = os.path.join(\"docker\",\"Benchmark\")\n",
    "    df = pd.read_csv(os.path.join(base_path,problem,\"results\",\"results_logic_final.csv\"))\n",
    "    return df\n",
    "\n",
    "def get_gnn_results(problem):\n",
    "    base_path = os.path.join(\"docker\",\"Benchmark\")\n",
    "    df = pd.read_csv(os.path.join(base_path,problem,\"results\",\"results_gnn_final.csv\"))\n",
    "    return df\n",
    "\n",
    "def get_kernel_results(problem):\n",
    "    base_path = os.path.join(\"docker\",\"Benchmark\")\n",
    "    df = pd.read_csv(os.path.join(base_path,problem,\"results\",\"kernel_results_final.csv\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN\n",
      "repr node_only\n",
      "68.67\\pm0.62\n",
      "repr node_edge\n",
      "69.31\\pm1.25\n",
      "repr edge_based\n",
      "67.87\\pm0.96\n",
      "repr Klog\n",
      "68.61\\pm1.18\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docker/Benchmark/mutag/results/results_logic_final_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mgrouped_test_acc[\u001b[38;5;28mrepr\u001b[39m][best_model],\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mgrouped_std[\u001b[38;5;28mrepr\u001b[39m][best_model],\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#df_logic = get_logic_results(dataset_name)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m df_logic \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBenchmark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults_logic_final_1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker/Benchmark/mutag/results/results_logic_final_1.csv'"
     ]
    }
   ],
   "source": [
    "# proces over the representations\n",
    "\n",
    "dataset_name = \"mutag\"\n",
    "#df_gnn = get_gnn_results(dataset_name)\n",
    "df_gnn = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset_name,\"results\",\"results_gnn_final.csv\"))\n",
    "\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"] \n",
    "#representations = [\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "\n",
    "# group the gnn test_acc by representation\n",
    "grouped_test_acc = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc\"].mean()\n",
    "grouped_std = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc_std\"].mean()\n",
    "\n",
    "print(\"GNN\")\n",
    "for repr in representations:\n",
    "    # best model for this representation\n",
    "    best_model = grouped_test_acc[repr].idxmax()\n",
    "    print(\"repr\",repr)\n",
    "    print(f\"{round(100*grouped_test_acc[repr][best_model],2)}\\pm{round(100*grouped_std[repr][best_model],2)}\")\n",
    "\n",
    "#df_logic = get_logic_results(dataset_name)\n",
    "df_logic = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset_name,\"results\",\"results_logic_final_1.csv\"))\n",
    "print(\"\\n\")\n",
    "print(\"Logic\")\n",
    "grouped_test_acc = df_logic.groupby(by=[\"representation\"])[\"test_acc\"].max()\n",
    "for repr in representations:\n",
    "    print(\"repr\",repr)\n",
    "    print(f\"{round(100*grouped_test_acc[repr],2)}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN: \n",
      "repr node_only\n",
      "GINE:    76.19\\pm2.91\n",
      "GCN:    73.02\\pm3.03\n",
      "GlobalAttentionNet:    74.6\\pm1.79\n",
      "SAGPool:    74.6\\pm1.81\n",
      "repr node_edge\n",
      "GINE:    82.54\\pm1.86\n",
      "GCN:    74.6\\pm2.02\n",
      "GlobalAttentionNet:    72.22\\pm1.51\n",
      "SAGPool:    73.02\\pm2.06\n",
      "repr edge_based\n",
      "GINE:    91.27\\pm2.03\n",
      "GCN:    72.22\\pm3.34\n",
      "GlobalAttentionNet:    72.22\\pm3.18\n",
      "SAGPool:    72.22\\pm3.45\n",
      "repr Klog\n",
      "GINE:    88.89\\pm1.58\n",
      "GCN:    89.68\\pm1.05\n",
      "GlobalAttentionNet:    88.89\\pm0.84\n",
      "SAGPool:    90.48\\pm1.33\n",
      "\n",
      "\n",
      "Logic: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_logic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m grouped_test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mdf_logic\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;129;01min\u001b[39;00m representations:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mrepr\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_logic' is not defined"
     ]
    }
   ],
   "source": [
    "# process over the models/systems per representation\n",
    "dataset_name = \"krk\"\n",
    "#df_logic = get_logic_results(dataset_name)\n",
    "#df_logic = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset_name,\"results\",\"results_logic.csv\"))\n",
    "#df_gnn = get_gnn_results(dataset_name)\n",
    "df_gnn = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset_name,\"results\",\"results_gnn_dist.csv\"))\n",
    "df_gnn = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset_name,\"results\",\"results_gnn_final.csv\"))\n",
    "\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"] #+ [\"FullBoard\",\"FullDiag\"] #+ [\"VirtualNode\"]\n",
    "#representations = [\"node_edge\"]\n",
    "# group the gnn test_acc by representation\n",
    "grouped_test_acc = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc\"].max()\n",
    "grouped_std = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc_std\"].mean()\n",
    "\n",
    "print(\"GNN: \")\n",
    "for repr in representations:\n",
    "    print(\"repr\",repr)\n",
    "    for model in df_gnn[\"model\"].unique():\n",
    "        print(f\"{model}:    {round(100*grouped_test_acc[repr][model],2)}\\pm{round(100*grouped_std[repr][model],2)}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Logic: \")\n",
    "grouped_test_acc = df_logic.groupby(by=[\"representation\",\"system\"])[\"test_acc\"].max()\n",
    "for repr in representations:\n",
    "    print(\"repr\",repr)\n",
    "    for system in df_logic[\"system\"].unique():\n",
    "        print(f\"{system}:    {round(100*grouped_test_acc[repr][system],2)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\midrule\n",
      "node_only &  &  &  &  &  & \\\\\n",
      "node_edge & $67.5\\pm0.0$ & $76.19\\pm0.0$ & $58.57\\pm0.0$ & $91.49\\pm0.0$ & $63.01\\pm0.0$ & \\\\\n",
      "edge_based & $67.5\\pm0.0$ & $66.67\\pm0.0$ & $60.48\\pm2.97$ &  & $54.79\\pm0.0$ & \\\\\n",
      "Klog & $67.5\\pm0.0$ & $78.57\\pm0.0$ & $51.43\\pm0.0$ & $91.49\\pm0.0$ & $62.56\\pm3.16$ & \\\\\n",
      "\\midrule\n",
      "node_only & \\\\\n",
      "node_edge & GS & WL & WL & WL & WL & \\\\\n",
      "edge_based & SP & WL & GS & SP & \\\\\n",
      "Klog & GS & WL & WL & WL & WL & \\\\\n"
     ]
    }
   ],
   "source": [
    "# graph kernel results\n",
    "\n",
    "dataset = \"train\"\n",
    "dataset = \"university\"\n",
    "dataset = \"bongard\"\n",
    "dataset = \"cyclic\"\n",
    "#datasets = [\"krk\",\"bongard\",\"train\",\"sameGen\",\"cyclic\"]\n",
    "datasets = [\"mutag\",\"nci\",\"cancer\",\"financial\",\"ptc\"]\n",
    "\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "#representations = [\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "\n",
    "# first go over the GNN representations\n",
    "print(\"\\midrule\")\n",
    "for repr in representations:\n",
    "    print(f\"{repr} & \",end=\"\")\n",
    "    for dataset in datasets:\n",
    "        df_gnn = get_kernel_results(dataset)\n",
    "        df_gnn_grouped = df_gnn.groupby(by=[\"representation\",\"graph_kernel\"])[\"test_accuracy\"].mean()\n",
    "        # get the best model\n",
    "        if repr in df_gnn_grouped:\n",
    "            best_model = df_gnn_grouped[repr].idxmax()\n",
    "            #print(\"best_model\",best_model)\n",
    "            if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                # take the second best model\n",
    "                best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[1]\n",
    "                if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                    # take the third best model\n",
    "                    best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[2]\n",
    "            \n",
    "            df_gnn_grouped_std = df_gnn.groupby(by=[\"representation\",\"graph_kernel\"])[\"test_accuracy\"].std()\n",
    "            # if last column, do not print the &\n",
    "            print(f\"${round(100*df_gnn_grouped[repr][best_model],2)}\\pm{round(100*df_gnn_grouped_std[repr][best_model],2)}$ & \",end=\"\")\n",
    "        else:\n",
    "            print(\" & \",end=\"\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "\n",
    "# print the best model for th graph kernels\n",
    "print(\"\\midrule\")\n",
    "for repr in representations:\n",
    "    print(f\"{repr} & \",end=\"\")\n",
    "    for dataset in datasets:\n",
    "        df_gnn = get_kernel_results(dataset)\n",
    "        df_gnn_grouped = df_gnn.groupby(by=[\"representation\",\"graph_kernel\"])[\"test_accuracy\"].mean()\n",
    "        # get the best model\n",
    "        if repr in df_gnn_grouped:\n",
    "            best_model = df_gnn_grouped[repr].idxmax()\n",
    "            if best_model == \"weisfeiler_lehman\":\n",
    "                best_model = \"WL\"\n",
    "            elif best_model == \"graphlet_sampling\":\n",
    "                best_model = \"GS\"\n",
    "            elif best_model == \"shortest_path\":\n",
    "                best_model = \"SP\"\n",
    "            # if last column, do not print the &\n",
    "            print(f\"{best_model} & \",end=\"\")\n",
    "    print(\"\\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\midrule\n",
      "GINE & $61.49\\pm2.34$ & $57.58\\pm1.32$ & $59.72\\pm2.3$ \\\\\n",
      "GCN & $62.5\\pm2.08$ & $59.22\\pm3.52$ & $59.6\\pm1.32$ \\\\\n",
      "GlobalAttentionNet & $61.87\\pm1.76$ & $57.45\\pm0.54$ & $60.98\\pm1.33$ \\\\\n",
      "SAGPool & $60.23\\pm2.27$ & $59.6\\pm2.31$ & $59.72\\pm1.67$ \\\\\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docker/Benchmark/cancer/results/results_logic_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mdf_logic_grouped[\u001b[38;5;28mrepr\u001b[39m][system],\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m$ & \u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mprint_latex_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcancer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mprint_latex_table\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Logic rows: the rows are the system and the columns the representations\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_logic \u001b[38;5;241m=\u001b[39m \u001b[43mget_logic_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmidrule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m system \u001b[38;5;129;01min\u001b[39;00m df_logic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mget_logic_results\u001b[0;34m(problem)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_logic_results\u001b[39m(problem):\n\u001b[1;32m      2\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocker\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults_logic_final.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/ILP/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker/Benchmark/cancer/results/results_logic_final.csv'"
     ]
    }
   ],
   "source": [
    "def print_latex_table(dataset):\n",
    "\n",
    "    # GNN rows: the rows are the model and the columns the representations (node_only, node_edge, edge_based, Klog)\n",
    "    df_gnn = get_gnn_results(dataset)\n",
    "    #df_gnn = pd.read_csv(os.path.join(\"docker\",\"Benchmark\",dataset,\"results\",\"results_gnn.csv\"))\n",
    "    df_gnn_grouped = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc\"].mean()\n",
    "    df_gnn_grouped_std = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc_std\"].mean()\n",
    "\n",
    "    representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "    representations = [\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "    #representations = [\"node_edge\",\"Klog\"]\n",
    "\n",
    "    print(\"\\midrule\")\n",
    "    for model in df_gnn[\"model\"].unique():\n",
    "        print(f\"{model} & \",end=\"\")\n",
    "        for repr in representations:\n",
    "            # if last column, do not print the &\n",
    "            if repr == representations[-1]:\n",
    "                print(f\"${round(100*df_gnn_grouped[repr][model],2)}\\pm{round(100*df_gnn_grouped_std[repr][model],2)}$ \",end=\"\")\n",
    "            else:\n",
    "                print(f\"${round(100*df_gnn_grouped[repr][model],2)}\\pm{round(100*df_gnn_grouped_std[repr][model],2)}$ & \",end=\"\")\n",
    "        print(\"\\\\\\\\\")\n",
    "\n",
    "    # Logic rows: the rows are the system and the columns the representations\n",
    "    df_logic = get_logic_results(dataset)\n",
    "\n",
    "    print(\"\\midrule\")\n",
    "    for system in df_logic[\"system\"].unique():\n",
    "        print(f\"{system} & \",end=\"\")\n",
    "        df_logic_grouped = df_logic.groupby(by=[\"representation\",\"system\"])[\"test_acc\"].max()\n",
    "        for repr in representations:\n",
    "            # if last column, do not print the &\n",
    "            if repr == representations[-1]:\n",
    "                print(f\"${round(100*df_logic_grouped[repr][system],2)}$ \",end=\"\")\n",
    "            else:\n",
    "                print(f\"${round(100*df_logic_grouped[repr][system],2)}$ & \",end=\"\")\n",
    "        print(\"\\\\\\\\\")\n",
    "    \n",
    "    \n",
    "\n",
    "print_latex_table(\"cancer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\midrule\n",
      "node_only & $68.67\\pm0.62$ & $76.46\\pm3.52$ &  &  & $64.21\\pm1.88$ & \\\\\n",
      "node_edge & $69.31\\pm1.25$ & $86.25\\pm3.63$ & $62.5\\pm2.08$ & $87.94\\pm0.75$ & $65.46\\pm1.68$ & \\\\\n",
      "edge_based & $67.87\\pm0.96$ & $88.75\\pm2.79$ & $59.6\\pm2.31$ &  & $64.49\\pm3.74$ & \\\\\n",
      "Klog & $68.61\\pm1.18$ & $88.12\\pm1.77$ & $60.98\\pm1.33$ & $87.59\\pm0.5$ & $65.22\\pm2.25$ & \\\\\n",
      "\\\\\n",
      "\\midrule\n",
      "node_only & GINE & GINE &  &  & GINE & \\\\\n",
      "node_edge & GINE & GINE & GCN & GINE & GINE & \\\\\n",
      "edge_based & GINE & GINE & SAGPool &  & GINE & \\\\\n",
      "Klog & GINE & GINE & GlobalAttentionNet & GINE & GINE & \\\\\n"
     ]
    }
   ],
   "source": [
    "# create the full latex table with the datasets as columns and the representations per method as rows\n",
    "\n",
    "datasets = [\"krk\",\"bongard\",\"train\",\"sameGen\",\"cyclic\"]\n",
    "datasets = [\"mutag\",\"nci\",\"cancer\",\"financial\",\"ptc\"]\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "\n",
    "\n",
    "# first go over the GNN representations\n",
    "print(\"\\midrule\")\n",
    "for repr in representations:\n",
    "    print(f\"{repr} & \",end=\"\")\n",
    "    for dataset in datasets:\n",
    "        df_gnn = get_gnn_results(dataset)\n",
    "        df_gnn_grouped = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc\"].mean()\n",
    "        # get the best model\n",
    "        if repr in df_gnn_grouped:\n",
    "            best_model = df_gnn_grouped[repr].idxmax()\n",
    "            if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                # take the second best model\n",
    "                best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[1]\n",
    "                if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                    # take the third best model\n",
    "                    best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[2]\n",
    "            \n",
    "            df_gnn_grouped_std = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc_std\"].mean()\n",
    "            # if last column, do not print the &\n",
    "            print(f\"${round(100*df_gnn_grouped[repr][best_model],2)}\\pm{round(100*df_gnn_grouped_std[repr][best_model],2)}$ & \",end=\"\")\n",
    "        else:\n",
    "            print(\" & \",end=\"\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "# then go over the logic representations\n",
    "# print(\"\\midrule\")\n",
    "# for repr in representations:\n",
    "#     print(f\"{repr} & \",end=\"\")\n",
    "#     for dataset in datasets:\n",
    "#         df_logic = get_logic_results(dataset)\n",
    "#         df_logic_grouped = df_logic.groupby(by=[\"representation\"])[\"test_acc\"].mean()\n",
    "#         # if last column, do not print the &\n",
    "#         if dataset == datasets[-1]:\n",
    "#             print(f\"${round(100*df_logic_grouped[repr],2)}$ \",end=\"\")\n",
    "#         else:\n",
    "#             print(f\"${round(100*df_logic_grouped[repr],2)}$ & \",end=\"\")\n",
    "#     print(\"\\\\\\\\\")\n",
    "\n",
    "# then go over the kernel representations\n",
    "# print(\"\\midrule\")\n",
    "# for repr in representations:\n",
    "#     print(f\"{repr} & \",end=\"\")\n",
    "#     for dataset in datasets:\n",
    "#         df = pd.read_csv(os.path.join(base_path,dataset,\"results\",\"kernel_results.csv\"))\n",
    "#         grouped_test_acc = df.groupby(by=[\"representation\",])[\"test_accuracy\"].max()\n",
    "#         grouped_std = df.groupby(by=[\"representation\"])[\"test_accuracy\"].std()\n",
    "#         # if last column, do not print the &\n",
    "#         if dataset == datasets[-1]:\n",
    "#             print(f\"${round(100*grouped_test_acc[repr],2)}\\pm{round(100*grouped_std[repr],2)}$ \",end=\"\")\n",
    "#         else:\n",
    "#             print(f\"${round(100*grouped_test_acc[repr],2)}\\pm{round(100*grouped_std[repr],2)}$ & \",end=\"\")\n",
    "#     print(\"\\\\\\\\\")\n",
    "\n",
    "\n",
    "# create a latex tabel that shows the best model or system per dataset and representation\n",
    "datasets = [\"krk\",\"bongard\",\"train\",\"sameGen\",\"cyclic\"]\n",
    "datasets = [\"mutag\",\"nci\",\"cancer\",\"financial\",\"ptc\"]\n",
    "representations = [\"node_only\",\"node_edge\",\"edge_based\",\"Klog\"]\n",
    "print(\"\\\\\\\\\")\n",
    "print(\"\\midrule\")\n",
    "\n",
    "for repr in representations:\n",
    "    print(f\"{repr} & \",end=\"\")\n",
    "    for dataset in datasets:\n",
    "        df_gnn = get_gnn_results(dataset)\n",
    "        df_gnn_grouped = df_gnn.groupby(by=[\"representation\",\"model\"])[\"test_acc\"].mean()\n",
    "        # get the best model\n",
    "        if repr in df_gnn_grouped:\n",
    "            best_model = df_gnn_grouped[repr].idxmax()\n",
    "            if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                # take the second best model\n",
    "                best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[1]\n",
    "                if df_gnn_grouped[repr][best_model] == 1.0:\n",
    "                    # take the third best model\n",
    "                    best_model = df_gnn_grouped[repr].sort_values(ascending=False).index[2]\n",
    "        else:\n",
    "            best_model = \"\"\n",
    "        \n",
    "        print(f\"{best_model} & \",end=\"\")\n",
    "    print(\"\\\\\\\\\")\n",
    "\n",
    "# print(\"\\midrule\")\n",
    "# for repr in representations:\n",
    "#     print(f\"{repr} & \",end=\"\")\n",
    "#     for dataset in datasets:\n",
    "#         df_logic = get_logic_results(dataset)\n",
    "#         df_logic_grouped = df_logic.groupby(by=[\"representation\",\"system\"])[\"test_acc\"].max()\n",
    "#         # if multiple systems have the same score take all of them\n",
    "#         best_systems = df_logic_grouped[repr][df_logic_grouped[repr] == df_logic_grouped[repr].max()].index\n",
    "#         # put the best systems in a string seperated by ,\n",
    "#         best_system = \",\".join(best_systems)\n",
    "#         print(f\"{best_system} & \",end=\"\")\n",
    "#     print(\"\\\\\\\\\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
